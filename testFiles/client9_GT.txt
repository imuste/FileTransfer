DNS is important for load balancing because google.com for example, has many IP addresses associated with it so that if many people access this website, they're not all going to the same server IP address. So the authoritative servers are responsible for doing this load balancing.
DNS uses UDP because we want low overhead and we don't want the root maintaining millions of connections that we might not even need. So we don't want any latency. On the application layer, we just keep trying to get to the correct server until we get it, so we build reliability into the application layer.
With this DNS approach, there is no one server that has everything, so the top level servers only know where to redirect the client but not the answer that they're looking for, this comes from the low level authoritative servers.
Besides DNS getting the IP address, it can also get the canonical hostname which is the real host name of a website that might be longer and more complicated than the one we use on a daily basis. So for example relay1.west-coast.yahoo.com might be the canonical hostname for yahoo.com .
In designing who a client gives and gets chunks from, we have to consider which chunks it wants to request and who to serve chunks to. We want to request chunks that we don't have yet and we want to use the rarest first strategy which means we want to get the rarest chunks first before they disappear. You also want even distribution of chunks in case that only a few others have certain chunks which when they're done, disappear which causes the chunks become less and less available.
When you pick nodes this is called unchocked those nodes and the random one is optimistic unchocked and the ones we don't pick are chocked. So in a stable state, the slowest nodes are the last ones to get the full download because the connections between nodes trickles down to the slower nodes.
The tracker has a hash map of the chunks and what they has to which can be used to confirm correctness of the info we're getting. The picking of nodes from random selection are nodes that aren't currently serving us. From nodes that are serving us, we will calculate the time it takes based on size and then we can see which peers are faster and rank those higher for the next round.
TL provides reliability, ordering, stream-oriented, multiplexing, demultiplexing, and data integrity. Multiplexing means that we can have multiple applications use the same network and we need to know which packets go to which applications and sockets. Data integrity refers to the bits in a packet being correct and not flipped which is important for the validity of the data.
Generally, delivering data in a transport layer to the correct socket is called demultiplexing, and the job of gathering data chunks at the source host from different sockets, encapsulating each data chunk with header information to create segments, and passing the segments to the network layer is called multiplexing. So the idea of demultiplexing is that if we have data coming in to the transport layer, it needs to determine which process or socket to send this data to. Multiplexing and demultiplexing ultimately is what allows the transport layer to pass data between the application and the network layer.